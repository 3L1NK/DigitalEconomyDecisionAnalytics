# -*- coding: utf-8 -*-
"""DEDA5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4feldfqRdIqo6wCKM6E-oWzkxrIXl6b

# **DEDA Unit 5** : Basic Statistics and Visualisation in Python

### **Generating Numbers**
"""

"""
NumPy offers the random module for random sampling. To generate random arrays use the randint() method. The size parameter specifies the dimension of the array. The choice() method also allows you to return an array of values.
"""

import numpy as np

# Set seed from comparability
np.random.seed(1) 

# Generate a 3*5 matrix that consists of the values (1, 4, 7, and 10)
X = np.random.choice([1, 4, 7, 10], size=(3, 5)) 
print(X)
# Generate a 20*20 matrix with uniformly distributed random integers between 0 to 50
A = np.random.randint(low=0, high=50, size=(20, 20))
print(A)

"""### **Scatter Plotting** """

import matplotlib.pyplot as plt

fig = plt.figure()  # Create a window to draw
ax = plt.axes()  # Create axes
ax.plot(np.arange(1,21), A, 'o')  # Plot the dots, using circle
ax.set_title('Random Number Plotting')  # Set figure title
plt.show()
# Save figure to a high quality
fig.savefig('random_scatter.png', dpi=300, transparent=True)
fig.clear()  # clear the figure

# Another plot: a time series
x = np.random.randn(1000)
# plt also provide a quick way to draw
plt.plot(x,'.')    # Try other styles like ‘+’
plt.title('Normal Distribution White Noise')
plt.xlabel('i')  # Putting label on the axes
plt.ylabel('x_i')
plt.draw()
plt.savefig('normal_dis_scatter.png', dpi=300)

"""### **Matrix Operations** """

"""
NumPy allows for many matrix operations.The inalg.inv() computes the (multiplicative) inverse of a matrix. The dot() method returns the dot product of two arrays. Identity() returns the identity array and eye() returns a  two-dimensional  array with ones on the diagonal and zeros elsewhere.
"""
# Note: A here is the random matrix from above

A_inv = np.linalg.inv(A) # Inverse matrix
print(A_inv)

dot_result = np.dot(A, A_inv) # Matrix multiplication operation
print(dot_result)

idn_matrix = np.identity(20) # Generate a 20*20 identity matrix
print(idn_matrix)

# Using .allclose() function to evaluate two matrices are equal within tolerance
np.allclose(dot_result, np.eye(20))  # True

"""### **Eigenvalues** """

"""
The transpose() function from Numpy can be used to calculate the transpose of a matrix. The linalg.eig() method computes the eigenvalues of a squared array while diag() extracts a diagonal or construct a diagonal array.
"""
A = [[3, 1],[1, 1]] # [[3, 1],[1, 1]] would create a 2x2 matrix

A_eig = np.linalg.eig(A)
# Now the Jordan decomposition A = Gamma*Lambda*Gamma^T
E_val = A_eig[0]
Gamma = A_eig[1]
Lambda = np.diag(E_val)

# Check the result, you might get something within numerical eps
AA = np.dot( np.dot(Gamma, Lambda), np.transpose(Gamma) )
print( np.allclose(AA, A) )  # True

# Calculation of the square root of A
Lambda12 = np.sqrt(Lambda)
A12 = np.dot( np.dot(Gamma, Lambda12), np.transpose(Gamma) )

"""### **Fourier Transformation** """

from PIL import Image
from numpy.fft import fft,ifft
import numpy as np

# Open the image by using Python Imaging Library(PIL)
image_before=Image.open(‘berlin_view.jpg')
# Decoding and encoding image to float number
image_int=np.fromstring(image_before.tobytes(), dtype=np.int8)
# Processing Fourier transform
fft_transformed=fft(image_int)
# Filter the lower frequency, i.e. employ a high pass
fft_transformed=np.where(np.absolute(fft_transformed) < 9e4,0,fft_transformed)
# Inverse Fourier transform
fft_transformed=ifft(fft_transformed)
# Keep the real part
fft_transformed=np.int8(np.real(fft_transformed))
# Output the image
image_output=Image.frombytes(image_before.mode, image_before.size, fft_transformed)
image_output.show()

"""### **Normal Distribution** """

"""
To create an inert active plot for the probability and cumulative density function of the normal distribution we use the Plotly package. 
"""
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import plotly.express as px
from ipywidgets import interact, interactive, fixed, interact_manual

# Set up figure size of plot 
plt.rcParams['figure.figsize'] = (10,8) # length and width 
plt.rcParams['figure.dpi'] = 120 # general box size

"""
We define the general environment of our plot: size, colour and style. 
"""

# Plot the cdf and pdf in one figure
def f(mu,sigma, colour):

  fig = plt.figure(figsize=(10, 8))

  fig.subplots_adjust( hspace = 0.3)
  fig.patch.set_facecolor('#eeefef')
  plt.style.use('classic')

  # upper plot: pdf
  plt.subplot(2, 1, 1)  # (rows, columns, which one)
  x_axis = np.linspace(mu - 3*sigma, mu + 3*sigma, 1000)
  plt.plot(x_axis, stats.norm.pdf(x_axis, mu, sigma), c= colour, linewidth= 2)
  plt.xlabel('X')
  plt.ylabel('pdf')
  plt.ylim(0,1)
  plt.xlim(-10,10)
  plt.title(f'Probability Density Function pdf for $\mu= {mu}$ and $\sigma= {round(sigma,2)}$', fontweight="bold")
  # lower plot: cdf
  plt.subplot(2, 1, 2)
  plt.plot(x_axis, stats.norm.cdf(x_axis, mu, sigma), c= colour, linewidth= 2)
  plt.xlabel('X')
  plt.ylabel('cdf') 
  plt.ylim(0,1)
  plt.xlim(-10,10)
  plt.title(f'Cumulative Density Function cdf for $\mu= {mu}$ and $\sigma= {round(sigma,2)}$', fontweight="bold")




colours = ['red', 'green', 'blue']

#interact(f, mu=1, sigma=(0.1, 10, 0.1), colour = colours)
interact(f, mu=(-10, 10,1), sigma=(0.5, 5, 0.5), colour = colours)
plt.savefig("filename.png", transparent=True)

"""### **Kerner Density Estimation**  """

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.neighbors import KernelDensity

# Code reference: http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html

# Set up figure size of plot 
plt.rcParams['figure.figsize'] = (10,8) # length and width 
plt.rcParams['figure.dpi'] = 120 # general box size

N = 200
np.random.seed(1)
# Create 2 normal distributed data set
norm_data_1 = np.random.normal(0, 1, int(0.3 * N))
norm_data_2 = np.random.normal(5, 1, int(0.7 * N))
norm_data = np.concatenate((norm_data_1, norm_data_2))

# Create x axis range
X_plot = np.linspace(-5, 10, 1000)
# Create linear combination of 2 normal distributed random variable
norm_linear = (0.3 * norm(0, 1).pdf(X_plot) + 0.7 * norm(5, 1).pdf(X_plot))

fig, ax = plt.subplots()

# Plot the real distribution
ax.fill(X_plot, norm_linear, fc='black', alpha=0.2, label='Linear combination')

# Use 3 different kernels to estimate
for kernel in ['gaussian', 'tophat', 'epanechnikov']:

    # Initial an object to use kernel function to fit data, bandwidth will    affect the result
    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(norm_data.reshape(-1, 1))

    # Evaluate the density model on the data
    log_dens = kde.score_samples(X_plot.reshape(-1, 1))
    ax.plot(X_plot, np.exp(log_dens), '-', label="kernel = '{0}'".format(kernel))

# Add text on the plot, position argument can be arbitrary
ax.text(6, 0.38, "N={0} points".format(N))
# Add a legend to the left outside of the plot 
ax.legend(loc='upper left', bbox_to_anchor = (1,0.5))
# Plot the random points, squeeze them into narrow space
ax.plot(norm_data, -0.005 - 0.01 * np.random.random(norm_data.shape[0]), '+k')
# Set x-axis y-axis limit to adjust the figure
ax.set_xlim(-4, 9)
ax.set_ylim(-0.03, 0.4)
fig.savefig('kernel_estimation.png', dpi=300, transparent = True, bbox_inches='tight')
plt.show()

"""### **Box-Muller Method on 2-dim normal distribution**  """

"""
Using the Box-Muller Method to generate 2-dim normally distributed variables. 
"""
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(100) 
# For mu = (0,0), covariance matrix Sigma = identity matrix
n = 500  # Number of random numbers
msize = 0.1  # determines the size of the plotted points
# a good size might be msize=5 for n=500 pts and msize=0.1 for n>50K  
a = np.random.exponential(scale=1, size=n)
phi = np.random.uniform(low=0, high=2 * np.pi, size=n)

# Change to cartesian coordinates
x = a * np.cos(phi)
y = a * np.sin(phi)

plt.figure(figsize=(7, 7))
plt.plot(x, y,'.r', markersize=msize)
plt.savefig('2dimnormal.png', dpi = 300, transparent = True)

"""
For covariance matrix Sigma = A: Y = X/sqrt(Sigma) ~ N(0,I) => Y*sqrt(Sigma)
"""
# Calculate sqrt(A) with Jordan decomposition
A = [[3, 1], [1, 1]]
A_eig = np.linalg.eig(A)
E_val = A_eig[0]
Gamma = A_eig[1]
Lambda = np.diag(E_val)
np.sqrt(Lambda)
Lambda12 = np.sqrt(Lambda)
A12 = np.dot(np.dot(Gamma, Lambda12), np.transpose(Gamma))

# Solve with matrix multiplication
c = [x, y]
tfxy = np.dot(A12, c)

plt.figure(2, figsize=(6, 4))
plt.plot(tfxy[0], tfxy[1], 'ro', markersize=msize)