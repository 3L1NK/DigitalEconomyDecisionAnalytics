{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEDAUnit4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWrdQsZHFqwZ"
      },
      "source": [
        "# **DEDA Unit 4**: Web Scraping Example - South China Morning Post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5AChi27X8Ts"
      },
      "source": [
        "\"\"\"\n",
        "Web Scraping encompasses any method allowing for extracting data from websites. Requests allows us to send an HTTP request to a webpage. BeautifulSoup parses the HTML in order to retrieve the desired information. \n",
        "Project: We wish to scrape the first page of the South China Morning Post’s news website. We acquire data about the news title, the news link and the news publication date and produce a tabular output stored as .csv file. \n",
        "\"\"\"\n",
        "\n",
        "# Load moduls \n",
        "import requests\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from datetime import datetime, date # needed to retrieve the date of publication \n",
        "\n",
        "# Receiving source code from the South China Morning Post website \n",
        "scmp_url = 'https://www.scmp.com/knowledge/topics/china-economy/news'\n",
        "url_request = requests.get(scmp_url)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1tivwOFE7CD"
      },
      "source": [
        "\"\"\"\n",
        "After accessing the HTML page, BeautifulSoup allows for ‘parsing’ the webpage source code, i.e. analysing its syntax. In this regard, regular expression is extensively used. \n",
        "\"\"\"\n",
        "\n",
        "# Returns the content of the response,  …\n",
        "url_content = url_request.content # … in bytes\n",
        "url_text = url_request.text # … in unicode\n",
        "\n",
        "# Using BeautifulSoup to parse webpage source code\n",
        "parsed_content = soup(url_content)\n",
        "parsed_text = soup(url_text, 'lxml') # ‘html.parser’ also possible \n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J7lcvhFFEMo",
        "outputId": "daedfcdf-ea86-46b5-e6d1-3f86c88db78d"
      },
      "source": [
        "\"\"\"\n",
        "There are some of the most versatile methods in BeautifulSoup: \n",
        "find_all() retrieves matching tags from all the nested HTML tags, which are called descendants. If a list is passed in, all matching objects will be retrieved. \n",
        "\"\"\"\n",
        "filtered_parts = parsed_text.find_all('div', class_ = \"sc-bdfBwQ cMShZj\") \n",
        "page_info = list() # create empty list \n",
        "\n",
        "# For loop iterates over every line in text \n",
        "for section in filtered_parts: \n",
        "    unit_info = dict() # create empty dictionary where we gradually attach items to\n",
        "\n",
        "    # (1) Filter title, link and text content \n",
        "    filtered_part1 = section.find_all('a', class_ = \"sc-hKgILt\") \n",
        "    # print(filtered_part1[0])\n",
        "    \n",
        "    # (2) Refilter to obtain date\n",
        "    filtered_part2 = section.find_all('span', class_ = 'sc-iqHYGH hkvqJt') # for date \n",
        "    # print(filtered_part2[0])\n",
        "\n",
        "    if filtered_part1 == []: # Error handling \n",
        "        continue\n",
        "\n",
        "    # (1) Filter title and link\n",
        "    news_title = filtered_part1[2].text.strip() # find title item\n",
        "    news_link = filtered_part1[0].get('href').strip() # find title item\n",
        "    news_link = f\"https://www.scmp.com{news_link}\" # adjust link\n",
        "\n",
        "    #print(news_title)\n",
        "    #print(news_link)\n",
        "\n",
        "    # Scanning scraped text (filtered_part1) for right titles \n",
        "    news_text = filtered_part1[3].text.strip() # find text \n",
        "    # print(news_link)\n",
        "\n",
        "    # Correcting title errors \n",
        "    if section == filtered_parts[0]:\n",
        "      news_title = news_title.replace(news_title, news_text) \n",
        "    for line in news_title: \n",
        "      if \"|\" in line: \n",
        "        news_title = f\"{news_title} {news_text}\"\n",
        "\n",
        "    unit_info['news_title'] = news_title # add news_title to the dictionary\n",
        "    unit_info['news_link'] = news_link # add news_link to the dictionary\n",
        "\n",
        "\n",
        "     # (2) Filter date \n",
        "    try:\n",
        "      news_date = datetime.strptime(filtered_part2[0].text.strip(),'%d %b %Y - %H:%M%p')\n",
        "      news_date = news_date.date() # cuts off the time \n",
        "      # print(news_date)\n",
        "    except:\n",
        "      t_day = date.today()\n",
        "      date_series = filtered_part2[0].text.strip()\n",
        "      if (date_series.endswith('hours ago')) or (date_series[1].startswith('minutes ago')):\n",
        "        news_date = t_day\n",
        "      else:\n",
        "        news_date = t_day\n",
        "    \n",
        "    unit_info['news_time'] = news_date # add news_date to the dictionary\n",
        "    page_info.append(unit_info) # attach dictionary to list\n",
        "print(page_info)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'news_title': 'Why is China’s middle class starting to look like the US’ not a good thing?', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158753/chinas-expanding-middle-class-starting-look-lot-us-its-not', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': '‘Risk of potential instability’ flagged as Beijing eyes annual economic meeting', 'news_link': 'https://www.scmp.com/economy/economic-indicators/article/3158789/ahead-chinas-key-economic-meeting-risk-potential', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'Top Beijing diplomat in Hong Kong urges US businesses to cooperate with China', 'news_link': 'https://www.scmp.com/news/hong-kong/politics/article/3158788/beijings-top-diplomat-hong-kong-calls-us-business-leaders', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'Beijing’s Big Tech crackdown takes back seat to economic stability in 2022', 'news_link': 'https://www.scmp.com/tech/policy/article/3158765/beijing-leaves-antitrust-out-2022-economic-goals-focuses-technological', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'China says cooperation on global trade key to post-pandemic recovery', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158769/china-says-worlds-multilateral-trade-system-key-post-pandemic', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': '‘Sustained rebound’ doubtful despite energy fuelling China’s import boom', 'news_link': 'https://www.scmp.com/economy/economic-indicators/article/3158712/china-trade-export-growth-slowed-november-imports', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'UK business group calls on Beijing to improve market reforms, resume flights', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158683/uk-business-group-calls-beijing-improve-market-reforms-and', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'Taiwan Strait tensions threaten to disrupt chip trade, global supply chains', 'news_link': 'https://www.scmp.com/tech/tech-war/article/3158673/us-china-tech-war-semiconductor-links-across-taiwan-strait-faces', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'China to cut banks’ reserve requirement ratio again to boost economic growth', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158649/china-cut-reserve-requirement-ratio-second-time-year-boost', 'news_time': datetime.date(2021, 12, 6)}, {'news_title': 'China’s green development plan for industrial sector lacks road map', 'news_link': 'https://www.scmp.com/business/china-business/article/3158643/chinas-five-year-green-development-plan-industrial-sector', 'news_time': datetime.date(2021, 12, 6)}, {'news_title': 'China’s grain output hits new high in 2021 amid food security drive', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158632/chinas-grain-output-hits-new-high-2021-food-security-drive', 'news_time': datetime.date(2021, 12, 6)}, {'news_title': 'More evidence China on high alert for monetary policy changes from US and EU', 'news_link': 'https://www.scmp.com/news/article/3158538/china-calls-better-international-consultation-over-monetary-policy-ensure', 'news_time': datetime.date(2021, 12, 6)}, {'news_title': 'Opinion| What both US and China get wrong on economic policy', 'news_link': 'https://www.scmp.com/comment/opinion/article/3158308/what-both-us-and-china-get-wrong-economic-policy-and-trade', 'news_time': datetime.date(2021, 12, 5)}, {'news_title': 'Explainer| Will China and six other emerging economies render the G7 redundant?', 'news_link': 'https://www.scmp.com/economy/global-economy/article/3158314/will-china-and-its-e7-emerging-economies-render-g7-redundant', 'news_time': datetime.date(2021, 12, 6)}, {'news_title': 'Time for a tax break for China’s Covid-hit SMEs: former mayor', 'news_link': 'https://www.scmp.com/news/article/3158531/time-permanent-tax-break-chinas-covid-hit-small-businesses-former-mayor-says', 'news_time': datetime.date(2021, 12, 5)}, {'news_title': 'Ignoring demand for foreign education, Beijing introduces tough curbs', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158333/chinas-middle-class-families-fret-president-xi-jinping', 'news_time': datetime.date(2021, 12, 5)}, {'news_title': '‘Everyone is frustrated’: international students wait in limbo for China to open', 'news_link': 'https://www.scmp.com/news/china/article/3158455/international-university-students-wait-china-remains-closed-them', 'news_time': datetime.date(2021, 12, 5)}, {'news_title': 'Rising cost of living leaves Chinese less willing, and less able, to consume', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158350/chinas-rising-living-costs-economic-uncertainties-leave-lower', 'news_time': datetime.date(2021, 12, 4)}, {'news_title': '‘Much earlier than expected’: China’s population set to peak in 2021', 'news_link': 'https://www.scmp.com/tech/policy/article/3158373/chinas-population-peak-2021-demographic-turning-point-has-already', 'news_time': datetime.date(2021, 12, 4)}, {'news_title': 'Fund sees bright future in solar after betting against private tutoring', 'news_link': 'https://www.scmp.com/tech/article/3158451/strategic-vision-fund-bets-solar-after-calling-chinas-private-tutoring', 'news_time': datetime.date(2021, 12, 4)}, {'news_title': 'Key China economic meeting set to start next week: sources', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158408/key-china-economic-planning-meeting-said-be-set-next-week', 'news_time': datetime.date(2021, 12, 7)}, {'news_title': 'China pledges ‘better business environment’ for foreign firms', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158317/china-pledges-fast-track-entry-foreign-workers-frustration', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'Boeing 737 MAX to return to China’s skies by ‘beginning of next year’', 'news_link': 'https://www.scmp.com/economy/global-economy/article/3158330/boeing-737-max-return-chinas-skies-beginning-next-year-more', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'Inflation, virus weigh on China’s services sector activity as expansion slows', 'news_link': 'https://www.scmp.com/economy/economic-indicators/article/3158258/chinas-services-sector-activity-expands-slower-pace', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'Hong Kong takes lead to help Hainan’s free-trade development', 'news_link': 'https://www.scmp.com/business/china-business/article/3158160/hong-kong-eyes-bigger-role-market-it-helps-hainan-free', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'China urged to boost ‘all possible’ global trade bonds in face of US pressure', 'news_link': 'https://www.scmp.com/economy/china-economy/article/3158209/chinese-think-tank-urges-beijing-strengthen-global-trade', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'US moves a step closer to delisting Chinese firms on American stock exchanges', 'news_link': 'https://www.scmp.com/news/world/united-states-canada/article/3158245/us-moves-step-closer-delisting-chinese-companies', 'news_time': datetime.date(2021, 12, 3)}, {'news_title': 'Yellen says cutting some tariffs on Chinese goods could ease price pressures', 'news_link': 'https://www.scmp.com/news/world/united-states-canada/article/3158239/us-treasury-secretary-janet-yellen-says-cutting', 'news_time': datetime.date(2021, 12, 2)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56YW3KwFkX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5dbd298-6323-403a-d33f-7f76fe222808"
      },
      "source": [
        "# Load moduls \n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "direct = os.getcwd()\n",
        "\n",
        "# Calling DataFrame constructor on our list \n",
        "df = pd.DataFrame(page_info, columns=['news_title', 'news_link', 'news_time'])\n",
        "print(df)\n",
        "# Exporting to .csv file \n",
        "df.to_csv(direct + '/CSMP_Scraped_News.csv')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           news_title  ...   news_time\n",
            "0   Why is China’s middle class starting to look l...  ...  2021-12-07\n",
            "1   ‘Risk of potential instability’ flagged as Bei...  ...  2021-12-07\n",
            "2   Top Beijing diplomat in Hong Kong urges US bus...  ...  2021-12-07\n",
            "3   Beijing’s Big Tech crackdown takes back seat t...  ...  2021-12-07\n",
            "4   China says cooperation on global trade key to ...  ...  2021-12-07\n",
            "5   ‘Sustained rebound’ doubtful despite energy fu...  ...  2021-12-07\n",
            "6   UK business group calls on Beijing to improve ...  ...  2021-12-07\n",
            "7   Taiwan Strait tensions threaten to disrupt chi...  ...  2021-12-07\n",
            "8   China to cut banks’ reserve requirement ratio ...  ...  2021-12-06\n",
            "9   China’s green development plan for industrial ...  ...  2021-12-06\n",
            "10  China’s grain output hits new high in 2021 ami...  ...  2021-12-06\n",
            "11  More evidence China on high alert for monetary...  ...  2021-12-06\n",
            "12  Opinion| What both US and China get wrong on e...  ...  2021-12-05\n",
            "13  Explainer| Will China and six other emerging e...  ...  2021-12-06\n",
            "14  Time for a tax break for China’s Covid-hit SME...  ...  2021-12-05\n",
            "15  Ignoring demand for foreign education, Beijing...  ...  2021-12-05\n",
            "16  ‘Everyone is frustrated’: international studen...  ...  2021-12-05\n",
            "17  Rising cost of living leaves Chinese less will...  ...  2021-12-04\n",
            "18  ‘Much earlier than expected’: China’s populati...  ...  2021-12-04\n",
            "19  Fund sees bright future in solar after betting...  ...  2021-12-04\n",
            "20  Key China economic meeting set to start next w...  ...  2021-12-07\n",
            "21  China pledges ‘better business environment’ fo...  ...  2021-12-03\n",
            "22  Boeing 737 MAX to return to China’s skies by ‘...  ...  2021-12-03\n",
            "23  Inflation, virus weigh on China’s services sec...  ...  2021-12-03\n",
            "24  Hong Kong takes lead to help Hainan’s free-tra...  ...  2021-12-03\n",
            "25  China urged to boost ‘all possible’ global tra...  ...  2021-12-03\n",
            "26  US moves a step closer to delisting Chinese fi...  ...  2021-12-03\n",
            "27  Yellen says cutting some tariffs on Chinese go...  ...  2021-12-02\n",
            "\n",
            "[28 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object-Oriented Programming (OOP)\n"
      ],
      "metadata": {
        "id": "7-Rvr7INUPj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OOP aims to structure the program by bundling related properties and characteristics into individual objects. Classes are required to build objects. To define a class we use the keyword 'class' and should start with a capital letter, according to PEP8. Classes defined functions called methods, which identify the behaviours and properties that an object created from the class can perform with its data. \n",
        "\"\"\"\n",
        "# The class Person is inherited from class object\n",
        "class Person(object):\n",
        "\n",
        "    # Using __init__ to initialize a class to take arguments\n",
        "    # self is default argument that points to the instance\n",
        "    def __init__(self, first, last, gender, age):\n",
        "        self.first_name = first\n",
        "        self.last_name = last\n",
        "        self.gender = gender\n",
        "        self.age = age\n"
      ],
      "metadata": {
        "id": "IRSCqc97UVUR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inheritance allows us to define a class that inherits all the methods and properties from another class. Hereby the class being inherited from is called base class or Parent class and the class that inherits from another class is referred to as derived class or Child class. Python provides the init method which creates instances automatically.\n",
        "\"\"\"\n",
        "\n",
        "class Student(Person):\n",
        "   # The class Student inherited from class Person\n",
        "    def __init__(self, first, last, gender, age, school):\n",
        "     # super() method allows us to handle the arguments from parent class without copying\n",
        "        super().__init__(first, last, gender, age)\n",
        "        # Child class can also be added new arguments\n",
        "        self.school = school\n",
        "    def describe(self):  # describe is a method of class Student\n",
        "        print('{0} {1} is a {2} years old {3} who studies at {4}.'.format( self.first_name,\n",
        "                                                                          self.last_name,\n",
        "                                                                          self.age,\n",
        "                                                                          self.gender,\n",
        "                                                                          self.school))\n",
        "        "
      ],
      "metadata": {
        "id": "He4wvDoxVF51"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# student1 is an instance of class Student\n",
        "student1 = Student('Laura', 'Doe', 'Female', 10 , 'C_School')\n",
        "\n",
        "print(issubclass(Student, Person))\n",
        "print(isinstance(student1, Student))\n",
        "\n",
        "# Using the attributes in the object student1\n",
        "print(student1.school)\n",
        "\n",
        "# Using the methods in the object student1\n",
        "print(student1.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iyXC1n5qWxF",
        "outputId": "6988239b-1996-4cc9-b1f5-7fefedd915eb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "C_School\n",
            "Laura Doe is a 10 years old Female who studies at C_School.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We now alter the code in ReadRSS.py file by using the class. There are some methods with name surrounded by double underscore called ‘magic method’. Further more see: https://www.python-course.eu/python3_magic_methods.php\n",
        "\"\"\"\n",
        "\n",
        "# !pip install feedparser\n",
        "import feedparser\n",
        "\n",
        "class ReadRSS(object):\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.response = feedparser.parse(self.url)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        # __eq__() is a magic method that enables comparison two object with ==\n",
        "        if self.url == other.url:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "    # __repr__() is a magic method that enables customization default printed format\n",
        "        return \"The website url is: \" + self.url\n",
        "\n",
        "    def get_titles(self):\n",
        "        titles = []\n",
        "        for item in self.response[\"entries\"]:\n",
        "            titles.append(item[\"title\"])\n",
        "        print(\"\\nTITLES:\\n\")\n",
        "        print('\\n'.join(titles))\n",
        "        return titles\n",
        "\n",
        "    def get_specificitem(self, item_name):\n",
        "        scripts = []\n",
        "        for item in self.response[\"entries\"]:\n",
        "            scripts.append(item[f\"{item_name}\"])\n",
        "        print(f\"\\n{item_name}:\\n\")\n",
        "        print('\\n'.join(scripts))\n",
        "        return scripts"
      ],
      "metadata": {
        "id": "eMRehf-aVcb9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReadRSSClass is the file name of the module code\n",
        "r = ReadRSS(\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\")\n",
        "r2 = ReadRSS(\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\")\n",
        "\n",
        "print(r)\n",
        "print(f'The type of r is: {type(r)}')\n",
        "\n",
        "if r == r2: # Here we use == to validate if two responses of two url are equal\n",
        "    print(\"Two urls are the same\")\n",
        "else:\n",
        "    print(\"Two urls are not the same\")\n",
        "# Print out the titles\n",
        "titles = r.get_titles()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOfOh31n4pWX",
        "outputId": "cd84daf1-912a-4cce-80aa-02ba8cd92efa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The website url is: https://feeds.a.dj.com/rss/RSSMarketsMain.xml\n",
            "The type of r is: <class '__main__.ReadRSS'>\n",
            "Two urls are the same\n",
            "\n",
            "TITLES:\n",
            "\n",
            "EPA Cuts Ethanol Requirement for Gasoline\n",
            "The 2021 Cash Gifts Guide: From Crypto to Tungsten Cubes\n",
            "What We Know About the Creator of Bitcoin\n",
            "Stocks, Oil Gain on Omicron Optimism\n",
            "Bitcoin's Selloff Driven by Options, but Also Interest Rates, Fed Policy\n",
            "Intel Will Race to Cash In Its Car Chips\n",
            "The Digital Car Is Coming. It Might Not Be More Profitable.\n",
            "Why the Turkish Lira Is Falling Like a Rock\n",
            "Investors Are Using Robinhood, Other Platforms to Jump Into Options Trades, Worrying U.S. Regulators\n",
            "Tesla, Intel, Bitcoin: What to Watch in the Stock Market Today\n",
            "Europe Tops China in Spawning $1 Billion Tech Startups\n",
            "Omicron Will Cause Problems, but Inflation May Not Be One\n",
            "Amazon Warehouses Are No Silver Bullet for Tired Shopping Malls\n",
            "Junk-Bond Investors Fear Bumpy 2022 After November Slump\n",
            "Omicron Sends Bank Stocks Seesawing\n",
            "Peter Lynch Donates Art Collection to Boston College\n",
            "Intel to List Shares in Mobileye Unit\n",
            "Trump, Lucid SPAC Deals Being Investigated by SEC\n",
            "Stocks Get a Boost as Omicron Concerns Ease\n",
            "BuzzFeed Shares Drop in Volatile First Trading Day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application** On Scraping Daily Weather Report of China Cities"
      ],
      "metadata": {
        "id": "1dbcTWaG5a46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a preliminary tutorial for scraping web pages\n",
        "With a lot of comments, one can easily get touch web scraping with Python\n",
        "Python Version: 3.6\n",
        "@Author: Junjie Hu, jeremy.junjie.hu@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "# Import all the packages you need, always remember that you can find 99% packages you need in python\n",
        "\n",
        "import requests  # take the website source code back to you\n",
        "import urllib  # some useful functions to deal with website URLs\n",
        "from bs4 import BeautifulSoup as soup  # a package to parse website source code\n",
        "import numpy as np  # all the numerical calculation related methods\n",
        "import re  # regular expression package\n",
        "import itertools  # a package to do iteration works\n",
        "import pickle  # a package to save your file temporarily\n",
        "import pandas as pd  # process structured data\n",
        "\n",
        "save_path = 'output/'  # the path you save your files\n",
        "\n",
        "base_link = 'http://www.tianqihoubao.com/lishi/'  # This link can represent the domain of a series of websites"
      ],
      "metadata": {
        "id": "Fm5DUFwh5bJd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def city_collection():\n",
        "    request_result = requests.get(base_link)  # get source code\n",
        "    parsed = soup(request_result.content)  # parse source code\n",
        "\n",
        "    dt_items = parsed.find_all('dt')  # find the items with tag named 'dt'\n",
        "    for item in dt_items:\n",
        "        # iterate within all the items\n",
        "        province_name = item.text.strip()  # get name of the province\n",
        "        province_link2cities = item.find('a')['href']  # get link to all the cities in the province\n",
        "        province = {'province_link': province_link2cities}\n",
        "        provinces[province_name] = province  # save dict in the dict\n",
        "\n",
        "    for province in provinces.keys():\n",
        "        # iterate with the province link to find all the cities\n",
        "        cities = {}\n",
        "        print(provinces[province]['province_link'])\n",
        "        request_province = requests.get(urllib.parse.urljoin(base_link, provinces[province]['province_link']))\n",
        "        # use the urllib package to join relative links in the proper way\n",
        "        parsed_province = soup(request_province.content)\n",
        "        dd_items = parsed_province.find_all('dd')\n",
        "        for dd_item in dd_items:\n",
        "            print(dd_item)\n",
        "            cities_items = dd_item.find_all('a')\n",
        "            for city_item in cities_items:\n",
        "                city_name = city_item.text.strip()\n",
        "                city_link = city_item.get('href').split('.')[0]\n",
        "                cities[city_name] = city_link\n",
        "        provinces[province]['cities'] = cities\n",
        "    return provinces"
      ],
      "metadata": {
        "id": "ciVnQ4MY5vBp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weather_collection(link):\n",
        "    \"\"\"\n",
        "    use the link to collect the weather data\n",
        "    :param link: url link\n",
        "    :return: dict, weather of a city everyday\n",
        "    \"\"\"\n",
        "    weather_page_request = requests.get(link)\n",
        "    parsed_page = soup(weather_page_request.content)\n",
        "    tr_items = parsed_page.find_all('tr')\n",
        "    month_weather = dict()\n",
        "    for tr_item in tr_items[1:]:\n",
        "        # print(tr_item)\n",
        "        # daily_weather = dict()\n",
        "        td_items = tr_item.find_all('td')\n",
        "        date = td_items[0].text.strip()\n",
        "        split_pattern = r'[\\n\\r\\s]\\s*'\n",
        "        weather_states = ''.join(re.split(split_pattern, td_items[1].text.strip()))\n",
        "        temperature = ''.join(re.split(split_pattern, td_items[2].text.strip()))\n",
        "        wind = ''.join(re.split(split_pattern, td_items[3].text.strip()))\n",
        "        month_weather[date] = {\n",
        "            'weather': weather_states,\n",
        "            'tempe': temperature,\n",
        "            'wind': wind\n",
        "        }\n",
        "        # month_weather.append(daily_weather)\n",
        "    return month_weather"
      ],
      "metadata": {
        "id": "cA3S4nYC5vxh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nice way to get a date string with certain format\n",
        "import datetime\n",
        "start_year = 2023\n",
        "end_year = 2024  # This is exclusive, so it will stop at 2023\n",
        "dates = ["
        "  (start_year + i // 12, i % 12 + 1)  # Calculate year and month\n"
        "    for i in range((end_year - start_year) * 12)\n"
        "]\n"
        "date = [\n"
        "    f"{year}{month:02d}"  # Format the date as 'YYYYMM'\n"
        "    for year, month in dates\n"
        "]\n"
        "\n",
        "#  ==== We have already download the links to all the cities=====\n",
        "#  ==== Otherwise, uncomment the function below to retrieve provinces information ======\n",
        "provinces = dict()  # initialize a dictionary to hold provinces information\n",
        "# This dictionary includes 'province_link' which is the links to find the cities for each province and the 'cities' contains city names and links\n",
        "\n",
        "# provinces_info = city_collection()  # Use this function to retrieve links to all the cities\n",
        "\n",
        "# This is called context management, with open can close the document automatically when the\n",
        "with open('output_cities_link.pkl', 'rb') as cities_file:  # write, change 'rb' -> 'wb'\n",
        "    provinces_info = pickle.load(cities_file)\n",
        "    print(provinces_info)\n",
        "    # pickle.dump(provinces_info, cities_file)  # write\n",
        "\n",
        "weather_record = dict()\n",
        "# The structure is dict in dict\n",
        "# first layer keyword is province name\n",
        "# In each province you can find the cities\n",
        "# In each city, you can find the date, in the date, you can find weather record"
      ],
      "metadata": {
        "id": "SDEI_OhQ57Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in provinces_info.keys():\n",
        "    # Iterate over different provinces\n",
        "    print(key)\n",
        "    for city_name, city_link in provinces_info[key]['cities'].items():\n",
        "        # Iterate cities within each provinces\n",
        "        print(city_name)\n",
        "        for month_date in date:\n",
        "            # Iterate over different months\n",
        "            print(city_name)\n",
        "            print(month_date)\n",
        "            print(provinces_info[key]['cities'][city_name])\n",
        "            print(\"On Scraping...\")\n",
        "            month_weather = weather_collection(\n",
        "                urllib.parse.urljoin(base_link, city_link) + '/month/' + month_date + '.html')\n",
        "            weather_record[key] = {city_name: {month_date: month_weather}}\n",
        "print('Finished Scraping.')\n",
        "\n",
        "# Exercise: Try to convert the \"json\"-like format to pandas DataFrame"
      ],
      "metadata": {
        "id": "jgkambJi5-KR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
